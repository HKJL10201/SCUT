import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

##æ„å»ºæ ‡å‡†å¾ªç¯ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹ä»»åŠ¡ 
#æ•°æ®ç”Ÿæˆï¼šä½¿ç”¨å‡½æ•°ğ‘¦ = ğ‘ ğ‘–ğ‘›(0.06ğ‘¥) + ğ‘ˆ(âˆ’0.1,0.1),ğ‘¥ = 1,2,â‹¯,600
num=600                     #600ä¸ªæ ·æœ¬ç‚¹
X=np.linspace(1,num,num)    #ğ‘¥ = 1,2,â‹¯,600
Y=np.sin(0.06*X)+np.random.uniform(-0.1,0.1)
#è®­ç»ƒé›†ï¼†æµ‹è¯•é›†æ ·æœ¬
divide=0.7      #åˆ†ç•Œç‚¹
y_train=Y[0:int(num*divide)]    #è®­ç»ƒé›†æ ·æœ¬ç‚¹ï¼šå‰é¢600 Ã— 0.7 = 420ä¸ªæ ·æœ¬ç‚¹ 
y_test=Y[int(num*divide):]      #æµ‹è¯•é›†æ ·æœ¬ç‚¹ï¼šåé¢600 Ã— 0.3 = 180ä¸ªæ ·æœ¬ç‚¹ 

#----------------------------------------------------------------------
batch_size = 1    #æ‰¹å¤„ç†å¤§å°
time_step = 7   #è¾“å…¥ç»´åº¦å¤§å°
hidden_dim = 32 #éšè—ç¥ç»å…ƒæ•°é‡
output_dim = 1  #è¾“å‡ºç»´åº¦

# è·å–æ•°æ®ï¼Œæ¯æ¬¡è·å–7ä¸ªæ ·æœ¬
inputs = tf.placeholder(tf.float32,shape=[time_step])
#å°†è¾“å…¥æ•°æ®è½¬æ¢æˆRNNè¾“å…¥æ ¼å¼[1,7,1]
x = tf.reshape(inputs,shape=[batch_size,time_step,output_dim])
#æ­£ç¡®æ ‡ç­¾çš„å ä½ç¬¦
labels = tf.placeholder(tf.float32,shape=[batch_size])
#å®šä¹‰RNNç»†èƒ
cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_dim)
#æ„å»º LSTM å¾ªç¯ç¥ç»ç½‘ç»œ
output1, state1 = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)
output2, state2 = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)
output3, state3 = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)

# æ„å»ºè¾“å‡ºå±‚
#dropoutå±‚
dropout1 = tf.layers.dropout(inputs=state1[1],rate=0.4)
dropout2 = tf.layers.dropout(inputs=state2[1],rate=0.4)
dropout3 = tf.layers.dropout(inputs=state3[1],rate=0.4)
#å…¨è¿æ¥å±‚
full_connected1 = tf.layers.dense(dropout1, 1)
full_connected2 = tf.layers.dense(dropout2, 1)
full_connected3 = tf.layers.dense(dropout3, 1)

#å®šä¹‰æŸå¤±
MSE1 = tf.reduce_sum(tf.square(full_connected1-labels))
MSE2 = tf.reduce_sum(tf.square(full_connected2-labels))
MSE3 = tf.reduce_sum(tf.square(full_connected3-labels))
#éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•
train_step1 = tf.train.GradientDescentOptimizer(0.01).minimize(MSE1)
#åŠ¨é‡æ¢¯åº¦ä¸‹é™ç®—æ³•
train_step2 = tf.train.MomentumOptimizer(0.01,0.1).minimize(MSE2)
# Adam ä¼˜åŒ–ç®—æ³•
train_step3 = tf.train.AdamOptimizer().minimize(MSE3)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    loss = [[],[],[]]
    predict = [[],[],[]]
    for i in range(int(num*divide) - time_step):    #åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒ
        _,sat = sess.run([train_step1, MSE1], feed_dict={inputs: y_train[i:i+time_step], labels: [y_train[i+time_step]]})
        loss[0].append(sat)
        _,sat = sess.run([train_step2, MSE2], feed_dict={inputs: y_train[i:i+time_step], labels: [y_train[i+time_step]]})
        loss[1].append(sat)
        _,sat = sess.run([train_step3, MSE3], feed_dict={inputs: y_train[i:i+time_step], labels: [y_train[i+time_step]]})
        loss[2].append(sat)
    for i in range(int(num*(1-divide)) - time_step):    #åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæµ‹è¯•
        _, sat = sess.run([train_step1, full_connected1], feed_dict={inputs: y_test[i:i+time_step], labels: [y_test[i + time_step]]})
        predict[0].append(sat[0][0])
        _, sat = sess.run([train_step2, full_connected2], feed_dict={inputs: y_test[i:i+time_step], labels: [y_test[i + time_step]]})
        predict[1].append(sat[0][0])
        _, sat = sess.run([train_step3, full_connected3], feed_dict={inputs: y_test[i:i+time_step], labels: [y_test[i + time_step]]})
        predict[2].append(sat[0][0])

#è®­ç»ƒè¯¯å·®éšè¿­ä»£æ¬¡æ•°çš„å˜åŒ–å›¾
plt.subplot(321)
plt.plot(X[0:413],loss[0])
#åŠæµ‹è¯•é›†çš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å¯¹æ¯”å›¾
plt.subplot(322)
plt.plot(X[427:],y_test[time_step:],'.')
plt.plot(X[427:],predict[0])
#è®­ç»ƒè¯¯å·®éšè¿­ä»£æ¬¡æ•°çš„å˜åŒ–å›¾
plt.subplot(323)
plt.plot(X[0:413],loss[1])
#åŠæµ‹è¯•é›†çš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å¯¹æ¯”å›¾
plt.subplot(324)
plt.plot(X[427:],y_test[time_step:],'.')
plt.plot(X[427:],predict[1])
#è®­ç»ƒè¯¯å·®éšè¿­ä»£æ¬¡æ•°çš„å˜åŒ–å›¾
plt.subplot(325)
plt.plot(X[0:413],loss[2])
#åŠæµ‹è¯•é›†çš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å¯¹æ¯”å›¾
plt.subplot(326)
plt.plot(X[427:],y_test[time_step:],'.')
plt.plot(X[427:],predict[2])
plt.show()